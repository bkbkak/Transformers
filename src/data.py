import os
import re
import torch
import torch.nn as nn
import sentencepiece as spm
import json
from torch.utils.data import Dataset, DataLoader

# ================== é…ç½® ==================
DATA_DIR = "./data/en-de-data"
LANG_PAIR = ("en", "de")
VOCAB_SIZE = 8000
SAVE_DIR = "../preprocessed"
os.makedirs(SAVE_DIR, exist_ok=True)


# ================== 1. è¯»å–æ–‡æœ¬ ==================
def load_ted_file(path):
    """è¯»å– train.tags.* æˆ– IWSLT17.TED.*.xml æ–‡ä»¶"""
    lines = []
    with open(path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # è·³è¿‡éå†…å®¹æ ‡ç­¾
            if line.startswith('<') and not line.startswith('<seg'):
                continue
            # æå– <seg id="x">...</seg>
            if '<seg' in line:
                m = re.search(r'<seg id="\d+">(.*?)</seg>', line)
                if m:
                    lines.append(m.group(1))
            elif not line.startswith('<'):
                lines.append(line)
    # print(f"ğŸ“„ Loaded {len(lines)} lines from {os.path.basename(path)}") # è®­ç»ƒæ—¶å¯ä»¥æ³¨é‡Šæ‰ï¼Œé¿å…å¹²æ‰°æ—¥å¿—
    return lines


def read_iwslt_split(split_name):
    """è¯»å– train/dev/test æ–‡ä»¶"""
    if split_name == "train":
        src_file = f"train.tags.de-en.{LANG_PAIR[0]}"
        tgt_file = f"train.tags.de-en.{LANG_PAIR[1]}"
    else:
        src_file = f"IWSLT17.TED.{split_name}.de-en.{LANG_PAIR[0]}.xml"
        tgt_file = f"IWSLT17.TED.{split_name}.de-en.{LANG_PAIR[1]}.xml"
    src_lines = load_ted_file(os.path.join(DATA_DIR, src_file))
    tgt_lines = load_ted_file(os.path.join(DATA_DIR, tgt_file))
    assert len(src_lines) == len(tgt_lines), f"{split_name}: æº/ç›®æ ‡å¥å­æ•°ä¸åŒ¹é…!"
    return src_lines, tgt_lines


# ================== 2. SentencePiece BPE ==================
def train_and_load_spm(sp_model_prefix, train_src, train_tgt):
    if not os.path.exists(sp_model_prefix + ".model"):
        print("ğŸš€ Training SentencePiece model...")
        temp_file = os.path.join(SAVE_DIR, "train_all.txt")
        with open(temp_file, "w", encoding="utf-8") as f:
            for line in train_src + train_tgt:
                f.write(line + "\n")
        
        # è€ƒè™‘åˆ°æŸäº›ç¯å¢ƒä¸­ç½‘ç»œé—®é¢˜ï¼Œè¿™é‡Œä½¿ç”¨try-exceptæ¥å¤„ç†å¯èƒ½å­˜åœ¨çš„os.removeå¤±è´¥
        try:
            spm.SentencePieceTrainer.Train(
                input=temp_file,
                model_prefix=sp_model_prefix,
                vocab_size=VOCAB_SIZE,
                model_type="bpe",
                pad_id=1, unk_id=0, bos_id=2, eos_id=3,
                character_coverage=1.0
            )
            os.remove(temp_file) # è®­ç»ƒå®Œååˆ é™¤ä¸´æ—¶æ–‡ä»¶
        except Exception as e:
            print(f"Error during SPM training: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
            return None

    sp = spm.SentencePieceProcessor(model_file=sp_model_prefix + ".model")
    print(f"âœ… Loaded SentencePiece model (vocab={sp.get_piece_size()})")
    return sp

# ================== 3. ç¼–ç ä¸ºå¼ é‡ ==================
def encode_split(src_lines, tgt_lines, sp, max_len=100):
    """æŠŠæ–‡æœ¬è½¬æˆå¼ é‡ (src, tgt_in, tgt_out)ï¼Œè‡ªåŠ¨ pad å¹¶è¿‡æ»¤ç©ºå¥"""
    src_ids, tgt_in_ids, tgt_out_ids = [], [], []

    for s, t in zip(src_lines, tgt_lines):
        if not s.strip() or not t.strip():
            continue  # è·³è¿‡ç©ºå¥
        
        # ç¡®ä¿åºåˆ—é•¿åº¦ä¸ä¼šè¶…è¿‡ max_len
        src_tensor = torch.tensor(sp.encode(s, out_type=int))
        tgt_tensor = torch.tensor(sp.encode(t, out_type=int))

        # ç›®æ ‡æ·»åŠ  <sos>, <eos>
        # é•¿åº¦æ£€æŸ¥ï¼šç¡®ä¿ SOS/EOS + å¥å­å†…å®¹ä¸ä¼šè¶…è¿‡ max_len
        actual_max_len = max_len - 1 # ç›®æ ‡åºåˆ—éœ€è¦å¤šä¸€ä¸ª EOS/SOS ä½

        # æˆªæ–­å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
        src_tensor = src_tensor[:actual_max_len] 
        tgt_tensor = tgt_tensor[:actual_max_len] 
        
        tgt_in = torch.cat([torch.tensor([sp.bos_id()]), tgt_tensor]) # tgt_in: <bos>, w1, w2, ..., wn
        tgt_out = torch.cat([tgt_tensor, torch.tensor([sp.eos_id()])]) # tgt_out: w1, w2, ..., wn, <eos>

        # pad
        def pad(t, target_len):
            if len(t) < target_len:
                return torch.cat([t, torch.full((target_len - len(t),), sp.pad_id())])
            else:
                return t[:target_len] 

        src_ids.append(pad(src_tensor, max_len))
        # ç›®æ ‡åºåˆ—ç»Ÿä¸€å¡«å……åˆ° max_len é•¿åº¦
        tgt_in_ids.append(pad(tgt_in, max_len))
        tgt_out_ids.append(pad(tgt_out, max_len))

    # print(f"  Encoded {len(src_ids)} samples")
    return torch.stack(src_ids), torch.stack(tgt_in_ids), torch.stack(tgt_out_ids)

# ================== 4. Dataset å’Œ DataLoader ==================
class TranslationDataset(Dataset):
    """è‡ªå®šä¹‰ PyTorch Dataset ç±»ï¼Œç”¨äºåŠ è½½é¢„å¤„ç†åçš„å¼ é‡"""
    def __init__(self, src, tgt_in, tgt_out):
        self.src = src
        self.tgt_in = tgt_in
        self.tgt_out = tgt_out
        assert len(src) == len(tgt_in) == len(tgt_out)

    def __len__(self):
        return len(self.src)

    def __getitem__(self, idx):
        return self.src[idx], self.tgt_in[idx], self.tgt_out[idx]

def load_data_and_get_dataloaders(batch_size, data_dir=SAVE_DIR):
    """åŠ è½½å·²ä¿å­˜çš„å¼ é‡æ–‡ä»¶ï¼Œå¹¶è¿”å› DataLoader"""
    try:
        # 1. åŠ è½½æ•°æ®å¼ é‡
        train_data = torch.load(os.path.join(data_dir, "train_data.pt"))
        dev_data = torch.load(os.path.join(data_dir, "dev_data.pt"))
        
        # 2. åˆ›å»º Dataset å®ä¾‹
        train_dataset = TranslationDataset(train_data['src'], train_data['tgt_in'], train_data['tgt_out'])
        dev_dataset = TranslationDataset(dev_data['src'], dev_data['tgt_in'], dev_data['tgt_out'])
        
        # 3. åˆ›å»º DataLoader å®ä¾‹
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        # dev è¯„ä¼°æ—¶ä¸éœ€è¦æ‰“ä¹±é¡ºåº
        dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)
        
        # 4. åŠ è½½è¯æ±‡è¡¨ä¿¡æ¯
        with open(os.path.join(data_dir, "vocab_info.json"), "r") as f:
            vocab_info = json.load(f)
            
        print(f"âœ… DataLoaders ready. Train size: {len(train_dataset)}, Dev size: {len(dev_dataset)}")
        return train_loader, dev_loader, vocab_info

    except FileNotFoundError as e:
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œé¢„å¤„ç†éƒ¨åˆ†: {e}")
        return None, None, None

# ================== 5. ä¸»æ‰§è¡Œé€»è¾‘ï¼šé¢„å¤„ç†å’Œä¿å­˜ ==================

if __name__ == '__main__':
    # å®šä¹‰è¦è®¾ç½®çš„ç¯å¢ƒå˜é‡åå’Œå€¼
    variable_name = "HF_ENDPOINT"
    variable_value = "https://hf-mirror.com"
    os.environ[variable_name] = variable_value
    
    # 1. è¯»å–åŸå§‹æ•°æ®
    train_src, train_tgt = read_iwslt_split("train")
    dev_src, dev_tgt = read_iwslt_split("dev2010")
    test_src, test_tgt = read_iwslt_split("tst2010")

    print(f"\nâœ… train: {len(train_src)} / {len(train_tgt)}")
    print(f"âœ… dev:   {len(dev_src)} / {len(dev_tgt)}")
    print(f"âœ… test:  {len(test_src)} / {len(test_tgt)}")

    # 2. è®­ç»ƒ SentencePiece æ¨¡å‹
    sp_model_prefix = os.path.join(SAVE_DIR, "spm_bpe_8k")
    sp = train_and_load_spm(sp_model_prefix, train_src, train_tgt)

    if sp:
        # 3. ç¼–ç ä¸ºå¼ é‡
        print("\nğŸ”„ Encoding train/dev/test splits...")
        train_tensors = encode_split(train_src, train_tgt, sp)
        dev_tensors = encode_split(dev_src, dev_tgt, sp)
        test_tensors = encode_split(test_src, test_tgt, sp)

        # 4. ä¿å­˜
        torch.save({"src": train_tensors[0], "tgt_in": train_tensors[1], "tgt_out": train_tensors[2]},
                   os.path.join(SAVE_DIR, "train_data.pt"))
        torch.save({"src": dev_tensors[0], "tgt_in": dev_tensors[1], "tgt_out": dev_tensors[2]},
                   os.path.join(SAVE_DIR, "dev_data.pt"))
        torch.save({"src": test_tensors[0], "tgt_in": test_tensors[1], "tgt_out": test_tensors[2]},
                   os.path.join(SAVE_DIR, "test_data.pt"))

        json.dump({
            "vocab_size": sp.get_piece_size(),
            "pad_id": sp.pad_id(),
            "unk_id": sp.unk_id(),
            "sos_id": sp.bos_id(),
            "eos_id": sp.eos_id(),
            "lang_pair": LANG_PAIR
        }, open(os.path.join(SAVE_DIR, "vocab_info.json"), "w"), ensure_ascii=False, indent=2)

        print("\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼è¾“å‡ºï¼š")
        print(f"  - {SAVE_DIR}/spm_bpe_8k.model")
        print(f"  - {SAVE_DIR}/train_data.pt, dev_data.pt, test_data.pt")
        print(f"  - {SAVE_DIR}/vocab_info.json")