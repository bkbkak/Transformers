import os
import json
import argparse
import torch
import torch.nn as nn
from tqdm import tqdm
import sentencepiece as spm
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from torch.utils.data import DataLoader

from model import Transformer

# ================== 1. å‚æ•°è§£æ ==================
def parse_args():
    parser = argparse.ArgumentParser(description="Transformer Evaluation and Generation.")
    
    parser.add_argument('--checkpoint_path', type=str, required=True, 
                        help='Path to the model checkpoint (.pt file) to be loaded.')
    parser.add_argument('--data_dir', type=str, default="../preprocessed", 
                        help='Directory for preprocessed data.')
    parser.add_argument('--save_dir', type=str, default="../results", 
                        help='Directory to save generated samples and results.')
    parser.add_argument('--max_len', type=int, default=100, 
                        help='Maximum sequence length for generation.')
    parser.add_argument('--batch_size', type=int, default=32, 
                        help='Batch size for evaluation.')
    parser.add_argument('--debug_print_limit', type=int, default=5, # å¢åŠ è°ƒè¯•å‚æ•°
                        help='Number of samples for which to print raw IDs and tokens for debugging.')

    # --- æ¨¡å‹æ¶æ„å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´) ---
    parser.add_argument('--d_model', type=int, default=512, help='Embedding dimension.')
    parser.add_argument('--num_heads', type=int, default=8, help='Number of attention heads.')
    parser.add_argument('--num_layers', type=int, default=6, help='Number of encoder/decoder layers.')
    parser.add_argument('--d_ff', type=int, default=2048, help='Feed-forward network dimension.')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate (will be ignored during eval).')
    parser.add_argument('--use_pos_enc', type=lambda x: (str(x).lower() == 'true'), default=True, 
                        help='Whether the model was trained with positional encoding.')

    args = parser.parse_args()
    return args

# ================== 2. æ–‡æœ¬ç”Ÿæˆå‡½æ•° (è´ªå©ªæœç´¢) ==================
@torch.no_grad()
def greedy_decode(model, src, src_mask, max_len, sp, device):
    """
    ä½¿ç”¨è´ªå©ªæœç´¢ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚
    """
    # ç¼–ç å™¨è¾“å‡º
    memory = model.encode(src, src_mask)
    
    # åˆå§‹åŒ–ç›®æ ‡åºåˆ— (åªåŒ…å« <bos> æ ‡è®°)
    batch_size = src.size(0)
    # ä½¿ç”¨ sp.bos_id() è·å– <bos> ID
    ys = torch.full((batch_size, 1), sp.bos_id(), dtype=torch.long, device=device)

    for i in range(max_len - 1): # -1 æ˜¯å› ä¸ºå·²ç»æœ‰ <bos> äº†
        
        tgt_in = ys # è§£ç å™¨è¾“å…¥å§‹ç»ˆæ˜¯å½“å‰å·²ç”Ÿæˆçš„æ‰€æœ‰ ID
        
        # ç›®æ ‡åºåˆ—æ©ç  (nopeak mask)
        tgt_mask = model.make_tgt_mask(tgt_in)
        out = model.decode(memory, src_mask, tgt_in, tgt_mask) 
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼šåªå…³æ³¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # prob: (batch_size, tgt_vocab)
        prob = model.generator(out[:, -1])
        
        # è´ªå©ªé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
        _, next_word = torch.max(prob, dim=1) # next_word: (batch_size)
        
        # æ‹¼æ¥æ–°çš„è¯æ±‡
        ys = torch.cat([ys, next_word.unsqueeze(-1)], dim=1)
        
        # å¦‚æœæ‰€æœ‰åºåˆ—éƒ½ç”Ÿæˆäº† <eos>ï¼Œå¯ä»¥æå‰åœæ­¢
        # ç¡®ä¿ next_word æ˜¯ Long ç±»å‹ï¼Œä»¥ä¸ sp.eos_id() æ¯”è¾ƒ
        if (next_word == sp.eos_id()).all():
            break
            
    return ys[:, 1:] # ç§»é™¤åˆå§‹çš„ <bos> æ ‡è®°

# ================== 3. ä¸»è¯„ä¼°é€»è¾‘ ==================
def main():
    args = parse_args()
    
    # è®¾ç½®ä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {DEVICE}")

    # 1. åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½æµ‹è¯•é›†ï¼‰
    try:
        # ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨
        test_data_path = os.path.join(args.data_dir, "test_data.pt")
        vocab_info_path = os.path.join(args.data_dir, "vocab_info.json")

        if not os.path.exists(test_data_path) or not os.path.exists(vocab_info_path):
            print(f"âŒ Cannot find preprocessed data in {args.data_dir}. Please run data.py first.")
            return

        test_data = torch.load(test_data_path)
        with open(vocab_info_path, "r") as f:
            vocab_info = json.load(f)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ TestDataset ç±»
        class TestDataset(torch.utils.data.Dataset):
            def __init__(self, data):
                self.src = data["src"]
                self.tgt_out = data["tgt_out"] # TGT_OUT æ˜¯çœŸå®çš„å‚è€ƒç¿»è¯‘
            def __len__(self): return len(self.src)
            # è¿”å›æºåºåˆ—å’Œç›®æ ‡å‚è€ƒåºåˆ—
            def __getitem__(self, idx): return self.src[idx], self.tgt_out[idx]
        
        test_set = TestDataset(test_data)
        # ç¡®ä¿ DataLoader èƒ½å¤Ÿå¤„ç†åºåˆ—
        test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)
        
        print(f"âœ… Loaded test set with {len(test_set)} samples.")

    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        return

    # 2. åŠ è½½ SentencePiece æ¨¡å‹å’Œè¯æ±‡ä¿¡æ¯
    sp_path = os.path.join(args.data_dir, "spm_bpe_8k.model")
    if not os.path.exists(sp_path):
        print(f"âŒ Cannot find SentencePiece model at {sp_path}.")
        return

    sp = spm.SentencePieceProcessor(model_file=sp_path)
    
    VOCAB_SIZE = vocab_info["vocab_size"]
    
    # === ID Information ===
    # ä½¿ç”¨ .get() æ–¹æ³•å°è¯•ä» vocab_info ä¸­è·å– IDã€‚
    # ä¸ºäº†é²æ£’æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ spm åº“è‡ªèº«çš„æ–¹æ³•è·å–ç‰¹æ®Š IDã€‚
    PAD_ID = vocab_info.get("pad_id", sp.pad_id())
    BOS_ID = vocab_info.get("bos_id", sp.bos_id())
    EOS_ID = vocab_info.get("eos_id", sp.eos_id())
    UNK_ID = vocab_info.get("unk_id", sp.unk_id())

    print(f"Special IDs used (from vocab_info or default): PAD={PAD_ID}, BOS={BOS_ID}, EOS={EOS_ID}, UNK={UNK_ID}")
    # ========================================

    # 3. åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡
    print("ğŸš€ Initializing model and loading checkpoint...")
    model = Transformer(
        src_vocab=VOCAB_SIZE, tgt_vocab=VOCAB_SIZE, 
        d_model=args.d_model, num_heads=args.num_heads, 
        num_layers=args.num_layers, d_ff=args.d_ff, 
        dropout=args.dropout, max_len=args.max_len,
        use_pos_enc=args.use_pos_enc 
    ).to(DEVICE)
    
    model.eval()
    
    try:
        model.load_state_dict(torch.load(args.checkpoint_path, map_location=DEVICE))
        print(f"âœ… Successfully loaded model from {args.checkpoint_path}")
    except Exception as e:
        print(f"âŒ Error loading checkpoint: {e}")
        return

    # 4. è¯„ä¼°å¾ªç¯ï¼šç”Ÿæˆæ–‡æœ¬å¹¶è®¡ç®— BLEU
    total_bleu_score = 0
    reference_count = 0
    generated_samples = []
    
    # ç”¨äºå¹³æ»‘ BLEU åˆ†æ•°è®¡ç®—
    chencherry = SmoothingFunction() 
    
    print("\nğŸ§  Starting Generation and BLEU Score Calculation...")
    
    # ç»Ÿä¸€æ–‡ä»¶åå‘½åè§„åˆ™
    checkpoint_name = os.path.basename(args.checkpoint_path).replace(".pt", "")
    sample_filename = f"generated_samples_{checkpoint_name}.txt"
    bleu_filename = f"bleu_score_{checkpoint_name}.json"

    sample_filepath = os.path.join(args.save_dir, sample_filename)

    with open(sample_filepath, "w", encoding="utf-8") as f:
        
        for src, tgt_out in tqdm(test_loader, desc="Generating"):
            src = src.to(DEVICE)
            
            # 1. ç¼–ç å™¨æ©ç 
            src_mask = model.make_src_mask(src) 
            
            # 2. ç”Ÿæˆç›®æ ‡åºåˆ— ID
            predicted_ids = greedy_decode(model, src, src_mask, args.max_len, sp, DEVICE)
            
            # 3. ç»Ÿè®¡å’Œè§£ç 
            # predicted_ids å·²ç»ç§»é™¤äº† <bos>
            for i, (pred_id_seq, ref_id_seq) in enumerate(zip(predicted_ids.cpu().tolist(), tgt_out.cpu().tolist())):
                
                # a. ID åºåˆ—è½¬ tokens (ç§»é™¤ PAD_ID)
                predicted_tokens = [sp.id_to_piece(idx) for idx in pred_id_seq if idx != PAD_ID]
                
                # b. æˆªæ–­ EOS ä¹‹åçš„éƒ¨åˆ†
                # ä½¿ç”¨ EOS token å­—ç¬¦ä¸²è¿›è¡ŒæŸ¥æ‰¾
                eos_token_str = sp.id_to_piece(EOS_ID)
                try:
                    eos_index = predicted_tokens.index(eos_token_str)
                    predicted_tokens = predicted_tokens[:eos_index]
                except ValueError:
                    pass # æ²¡æœ‰æ‰¾åˆ° EOS
                
                # c. é¢„æµ‹æ–‡æœ¬
                predicted_text = sp.decode(predicted_tokens)

                # d. å¤„ç†å‚è€ƒåºåˆ— (reference_tokens): ç§»é™¤æ‰€æœ‰ç‰¹æ®Š token
                special_ids = {PAD_ID, BOS_ID, EOS_ID, UNK_ID}
                reference_tokens = [sp.id_to_piece(idx) for idx in ref_id_seq if idx not in special_ids]
                reference_text = sp.decode(reference_tokens)
                
                # e. BLEU åˆ†æ•°è®¡ç®— (ä½¿ç”¨ tokenized åºåˆ—)
                if reference_tokens:
                    # sentence_bleu æœŸæœ›å‚è€ƒï¼ˆreferenceï¼‰æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œä½†å¯¹äºå•ä¸ªå‚è€ƒï¼Œ
                    # ä¹Ÿå¯ä»¥æ¥å—ä¸€ä¸ªåŒ…å« token åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæˆ–è€…ç›´æ¥æ˜¯ token åˆ—è¡¨ã€‚
                    # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ [reference_tokens]
                    bleu = sentence_bleu([reference_tokens], predicted_tokens, 
                                         smoothing_function=chencherry.method1)
                    total_bleu_score += bleu
                    reference_count += 1
                else:
                    bleu = 0.0
                
                # f. å†™å…¥æ ·æœ¬
                sample_entry = {
                    "reference": reference_text,
                    "prediction": predicted_text,
                    "bleu": f"{bleu:.4f}"
                }
                generated_samples.append(sample_entry)

                # g. *** DEBUGGING OUTPUT (æ‰“å°åŸå§‹ ID) ***
                if len(generated_samples) <= args.debug_print_limit:
                    
                    # ä»…ç§»é™¤ PAD ID çš„åŸå§‹ç”Ÿæˆ ID åºåˆ—
                    raw_gen_ids = [idx for idx in pred_id_seq if idx != PAD_ID]
                    raw_gen_tokens = [sp.id_to_piece(idx) for idx in raw_gen_ids]
                    
                    print(f"\n--- DEBUG SAMPLE {len(generated_samples)} ---")
                    print(f"Source ID Sequence (First 10): {src[i].cpu().tolist()[:10]}...")
                    print(f"Reference Text: {reference_text}")
                    print(f"Generated Text: {predicted_text}")
                    print(f"RAW Generated IDs (First 20, no PAD): {raw_gen_ids[:20]}...")
                    print(f"RAW Generated Tokens (First 20, no PAD): {raw_gen_tokens[:20]}...")
                    
                
                if len(generated_samples) <= 20:
                    f.write("--- Sample ---\n")
                    f.write(f"Ref: {reference_text}\n")
                    f.write(f"Gen: {predicted_text}\n")
                    f.write(f"BLEU: {bleu:.4f}\n\n")
        
        print(f"\nğŸ’¾ Generated first 20 samples saved to {sample_filepath}")


    # 5. ç»“æœæ€»ç»“
    avg_bleu = total_bleu_score / reference_count if reference_count > 0 else 0.0
    print(f"\n==========================================")
    print(f"  Evaluation Complete. Total Samples: {reference_count}")
    print(f"  Average BLEU Score (Smoothed): {avg_bleu:.4f}")
    print(f"==========================================")
    
    # 6. ä¿å­˜æœ€ç»ˆBLEUåˆ†æ•°ï¼ˆç”¨äºç»“æœè¡¨æ ¼ï¼‰
    result_data = {
        "model_name": checkpoint_name,
        "average_bleu": avg_bleu, 
        "total_samples": reference_count
    }
    bleu_filepath = os.path.join(args.save_dir, bleu_filename)
    with open(bleu_filepath, "w") as f:
        json.dump(result_data, f, indent=4)
    print(f"ğŸ’¾ BLEU score saved to {bleu_filepath}")


if __name__ == '__main__':
    # ç¡®ä¿ results ç›®å½•å­˜åœ¨ï¼Œç»Ÿä¸€ç®¡ç†ç»“æœ
    os.makedirs("../results", exist_ok=True) 
    main()
